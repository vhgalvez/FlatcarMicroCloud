Documento: Soluci√≥n de Problemas de Conectividad en FlatcarMicroCloud mediante Open vSwitch y Configuraci√≥n de Red Bridge

Fecha: 23 de abril de 2025
Ubicaci√≥n: Madrid, Espa√±a
Autor: (Tu Nombre/Usuario)

1. Identificaci√≥n del Problema:

Se detectaron problemas de conectividad significativos en el entorno FlatcarMicroCloud, espec√≠ficamente desde los nodos master de Kubernetes (10.17.4.0/24). Estos problemas se manifestaron como la incapacidad de alcanzar varios recursos y servicios esenciales:

192.168.0.50 (k8s-api-lb): P√©rdida total de paquetes, lo que imped√≠a la comunicaci√≥n con el balanceador de carga de la API de Kubernetes.
10.17.3.11 (freeipa1): Fallo con error "Destination Port Unreachable", indicando un problema de enrutamiento o firewall hacia el servidor DNS y de autenticaci√≥n.
192.168.0.1 (router LAN): P√©rdida total de paquetes, lo que aislaba los nodos de la red local.
8.8.8.8 (DNS p√∫blico): P√©rdida total de paquetes, impidiendo la resoluci√≥n de nombres externa y el acceso a Internet.
192.168.0.55 (IP virtual de la API): P√©rdida total de paquetes, lo que dificultaba el acceso a la API de Kubernetes a trav√©s del balanceador de carga.
Estos problemas de conectividad apuntaban a una configuraci√≥n de red subyacente que no permit√≠a el tr√°fico adecuado entre las diferentes redes virtuales gestionadas por Libvirt y la red local (LAN). La configuraci√≥n inicial utilizaba el modo NAT para las redes virtuales, lo que introduc√≠a complejidad en el enrutamiento y posibles problemas de doble NAT.

2. Solicitud de Soluci√≥n:

La solicitud era establecer una comunicaci√≥n fluida y directa entre los diferentes componentes del entorno FlatcarMicroCloud, incluyendo los nodos Kubernetes, el balanceador de carga de la API, el servidor DNS/autenticaci√≥n y la red local. El objetivo principal era eliminar los problemas de conectividad que imped√≠an la correcta operaci√≥n del cl√∫ster Kubernetes y otros servicios.

3. Soluci√≥n Implementada: Configuraci√≥n de Red Bridge con Open vSwitch:

Para solucionar los problemas de conectividad, se implement√≥ la siguiente estrategia utilizando Open vSwitch (OVS) para gestionar las redes virtuales y facilitar la comunicaci√≥n a nivel de capa 2:

3.1. Instalaci√≥n de Open vSwitch:

En el sistema host (Rocky Linux), se instal√≥ Open vSwitch y sus servicios:

Bash

sudo dnf install openvswitch openvswitch-services
sudo systemctl enable --now openvswitch-services
sudo systemctl status openvswitch-services
3.2. Reconfiguraci√≥n de las Redes Libvirt en Modo Bridge:

Se modificaron los archivos de configuraci√≥n de Terraform para las redes Libvirt involucradas (br0, kube_network_02, kube_network_03) para utilizar el modo bridge y conectarse a un puente OVS (br0 existente):

br0_network/main.tf:

Terraform

resource "libvirt_network" "br0" {
  name      = var.rocky9_network_name
  mode      = "bridge"
  bridge    = "br0"
  autostart = true
  # addresses = ["10.17.5.0/24"] # Eliminado o comentado
}
nat_network_02/main.tf:

Terraform

resource "libvirt_network" "kube_network_02" {
  name      = "kube_network_02"
  mode      = "bridge"
  bridge    = "br0"
  autostart = true
  # addresses = ["10.17.3.0/24"] # Eliminado o comentado
}
nat_network_03/main.tf:

Terraform

resource "libvirt_network" "kube_network_03" {
  name      = "kube_network_03"
  mode      = "bridge"
  bridge    = "br0"
  autostart = true
  # addresses = ["10.17.4.0/24"] # Eliminado o comentado
}
Se aplicaron los cambios con terraform apply en cada directorio.

3.3. Configuraci√≥n del Puente OVS (br0):

Se hizo que Open vSwitch gestionara el puente br0 existente y se conect√≥ la interfaz f√≠sica del host a este puente:

Bash

sudo ovs-vsctl add-port br0 <nombre_de_la_interfaz_fisica>
sudo ip addr flush dev <nombre_de_la_interfaz_fisica>
sudo systemctl restart NetworkManager
Se verific√≥ y/o configur√≥ la direcci√≥n IP del puente br0 para asegurar la conectividad con la LAN.

3.4. Configuraci√≥n de nftables (Firewall):

Se modificaron las reglas de nftables en el host para permitir el tr√°fico entre las redes bridge y la LAN:

Fragmento de c√≥digo

#!/usr/sbin/nft -f

flush ruleset

# üëÆ Tabla de filtrado
table inet filter {
    chain input {
        type filter hook input priority 0; policy accept;

        # ... (reglas de entrada existentes) ...
        iifname { "virbr0", "virbr_kube01", "virbr_kube02", "virbr_kube03", "br0" } accept
        drop
    }

    chain forward {
        type filter hook forward priority 0; policy drop;

        # ‚úÖ Permitir tr√°fico entre todas las subredes internas (10.17.0.0/16)
        ip saddr 10.17.0.0/16 ip daddr 10.17.0.0/16 accept

        # ‚úÖ Permitir tr√°fico entre la LAN (192.168.0.0/24) y las subredes internas
        ip saddr 192.168.0.0/24 ip daddr 10.17.0.0/16 accept
        ip saddr 10.17.0.0/16 ip daddr 192.168.0.0/24 accept

        # ‚úÖ Permitir tr√°fico que pasa por el puente br0
        bridge iifname "br0" accept
        bridge oifname "br0" accept

        # ... (otras reglas de forward existentes) ...
    }

    chain output {
        type filter hook output priority 0; policy accept;
    }
}

# üîÑ Tabla de NAT
table inet nat {
    chain postrouting {
        type nat hook postrouting priority 100; policy accept;

        # ... (reglas de masquerade existentes) ...
    }
}
Se aplicaron las nuevas reglas con sudo nft -f /etc/nftables/flatcar-microcloud.conf y se habilit√≥ el servicio nftables.

4. Resultado de la Soluci√≥n:

Despu√©s de implementar la configuraci√≥n de red bridge con Open vSwitch y ajustar las reglas del firewall, se espera que los problemas de conectividad iniciales se resuelvan. Los nodos master de Kubernetes (10.17.4.0/24) deber√≠an ahora poder comunicarse directamente con:

192.168.0.50 (k8s-api-lb): Ya que todas las redes est√°n en el mismo segmento de capa 2 (el puente br0).
10.17.3.11 (freeipa1): De manera similar, la comunicaci√≥n directa deber√≠a ser posible.
192.168.0.1 (router LAN): La conectividad con la LAN se establece a trav√©s del puente br0.
8.8.8.8 (DNS p√∫blico): El tr√°fico ahora deber√≠a poder salir a Internet a trav√©s de la interfaz f√≠sica conectada al puente br0, sujeto a las reglas de NAT en nftables.
192.168.0.55 (IP virtual de la API): La comunicaci√≥n con la VIP en la LAN deber√≠a ser directa.
5. Documentaci√≥n del Error Original:

El error original radicaba en la configuraci√≥n de las redes virtuales en modo NAT, lo que creaba barreras de comunicaci√≥n y requer√≠a un enrutamiento complejo que no estaba configurado correctamente. El "Destination Port Unreachable" al intentar hacer ping a freeipa1 era un s√≠ntoma de este problema de enrutamiento a nivel del gateway NAT de la red 10.17.4.0/24. La falta de conectividad con la LAN y el exterior tambi√©n era una consecuencia de esta configuraci√≥n aislada y la falta de reglas de reenv√≠o adecuadas.

6. Conclusi√≥n:

La transici√≥n al modo de red bridge utilizando Open vSwitch proporciona una arquitectura de red m√°s plana y directa para el entorno FlatcarMicroCloud. Al conectar todas las redes virtuales relevantes al mismo puente (br0), se elimina la necesidad de NAT para la comunicaci√≥n interna y con la LAN. La correcta configuraci√≥n del puente OVS y las reglas de firewall en el host son cruciales para asegurar que el tr√°fico se reenv√≠e correctamente y que se restablezca la conectividad entre todos los componentes del entorno. Se recomienda verificar la conectividad despu√©s de aplicar esta configuraci√≥n realizando nuevamente las pruebas de ping desde los nodos master.



Documento: An√°lisis de la Ra√≠z del Problema de Conectividad en FlatcarMicroCloud (Pre-Implementaci√≥n de Open vSwitch Bridge)

Fecha: 23 de abril de 2025
Ubicaci√≥n: Rivas-Vaciamadrid, Comunidad de Madrid, Espa√±a
Autor: (Tu Nombre/Usuario)

1. Descripci√≥n del Error Original:

El entorno FlatcarMicroCloud experimentaba problemas significativos de conectividad, evidenciados por la incapacidad de los nodos master de Kubernetes (10.17.4.0/24) para comunicarse con varios recursos esenciales. Los s√≠ntomas principales inclu√≠an:

Aislamiento de la red LAN (192.168.0.0/24): Los pings a direcciones IP en la LAN (incluyendo el gateway y el balanceador de carga de la API) fallaban consistentemente.
Inaccesibilidad al servidor DNS/Autenticaci√≥n (10.17.3.11): Los intentos de comunicaci√≥n resultaban en errores de "Destination Port Unreachable", lo que suger√≠a un bloqueo a nivel de enrutamiento o firewall.
Falta de conectividad externa (Internet): Los pings a servidores DNS p√∫blicos fallaban, indicando que los nodos no pod√≠an alcanzar la red externa.
2. An√°lisis de la Configuraci√≥n de Red Original:

La configuraci√≥n de red inicial se basaba en el uso del modo Network Address Translation (NAT) para las redes virtuales gestionadas por Libvirt:

kube_network_02 (10.17.3.0/24): Aloja freeipa1, loadbalancer1, loadbalancer2, postgresql1. En modo NAT, Libvirt actuaba como un enrutador para esta red, traduciendo las direcciones IP internas de las VMs a la direcci√≥n IP del host para la comunicaci√≥n externa.
kube_network_03 (10.17.4.0/24): Aloja los nodos master, worker y storage1. Similar a kube_network_02, Libvirt gestionaba el NAT para esta red.
br0 (10.17.5.0/24): Aloja k8s-api-lb. Tambi√©n configurada en modo NAT por Libvirt.
3. Identificaci√≥n de la Ra√≠z del Problema:

La ra√≠z de los problemas de conectividad resid√≠a en la naturaleza aislada de las redes NAT y la falta de una configuraci√≥n de enrutamiento expl√≠cita y bidireccional entre estas redes y la red LAN.

Aislamiento de Redes NAT: En el modo NAT, cada red virtual creada por Libvirt opera en su propio espacio de direcciones IP aislado. Por defecto, no existe una ruta autom√°tica para que el tr√°fico se mueva entre diferentes redes NAT gestionadas por Libvirt o entre una red NAT y la red f√≠sica subyacente (LAN).

Falta de Enrutamiento: Para que la comunicaci√≥n sea posible entre diferentes subredes IP (como 10.17.4.0/24 y 192.168.0.0/24 o 10.17.3.0/24), se deben configurar rutas expl√≠citas a nivel del sistema host (donde se ejecuta Libvirt) o a trav√©s de un enrutador en la red f√≠sica. En la configuraci√≥n original, estas rutas no estaban adecuadamente establecidas.

"Destination Port Unreachable" a freeipa1: Este error espec√≠fico sugiere que el tr√°fico desde la red 10.17.4.0/24 llegaba al gateway NAT de esa red (la interfaz virtual de Libvirt para kube_network_03, probablemente 10.17.4.1), pero este gateway no ten√≠a una ruta definida para alcanzar la red 10.17.3.0/24 donde reside freeipa1, o el firewall en el gateway estaba bloqueando el tr√°fico.

Falta de Conectividad Externa: La comunicaci√≥n con Internet (a trav√©s de 8.8.8.8) fallaba porque, aunque la tabla NAT de nftables estaba configurada para realizar masquerade del tr√°fico saliente desde las redes 10.17.x.x hacia la interfaz f√≠sica del host, el tr√°fico originado en estas redes no estaba siendo correctamente enrutado hacia esa interfaz f√≠sica para salir.

Problemas con la LAN: La incapacidad de alcanzar direcciones en la LAN (192.168.0.0/24) desde las redes 10.17.x.x se deb√≠a a la falta de rutas de reenv√≠o configuradas en el host que permitieran el tr√°fico entre estos rangos de IP.

4. Conclusi√≥n sobre la Ra√≠z del Problema:

La ra√≠z de los problemas de conectividad en la configuraci√≥n original de FlatcarMicroCloud resid√≠a en la dependencia del modo NAT para las redes virtuales sin la configuraci√≥n de enrutamiento expl√≠cita necesaria para permitir la comunicaci√≥n entre estas redes y la red LAN. El modo NAT, por su naturaleza, crea l√≠mites de red aislados, y superar estos l√≠mites requiere una configuraci√≥n de enrutamiento y firewall adecuada a nivel del host o de un enrutador externo. La falta de esta configuraci√≥n result√≥ en el aislamiento de los nodos Kubernetes y la incapacidad de comunicarse con servicios esenciales y la red externa.

La soluci√≥n de implementar Open vSwitch en modo bridge aborda directamente esta ra√≠z del problema al eliminar la capa de NAT para la comunicaci√≥n interna y con la LAN, permitiendo una comunicaci√≥n m√°s directa a nivel de capa 2. Esto simplifica significativamente el enrutamiento y elimina la necesidad de complejas reglas de reenv√≠o de puertos que son t√≠picas en entornos con m√∫ltiples capas de NAT.


‚ÄãPara instalar Open vSwitch en AlmaLinux 9.5, necesitas habilitar el repositorio del SIG de NFV de CentOS, ya que los paquetes no est√°n disponibles por defecto. Sigue estos pasos:‚Äã

Habilita el repositorio NFV OpenvSwitch:

bash
Copiar
Editar
sudo dnf install -y centos-release-nfv-openvswitch
Instala Open vSwitch:

bash
Copiar
Editar
sudo dnf install -y openvswitch2.17
Nota: El paquete se llama openvswitch2.17 en este repositorio.

Habilita y arranca el servicio:

bash
Copiar
Editar
sudo systemctl enable --now openvswitch
Este procedimiento ha sido validado en entornos similares y se ha documentado en foros especializados. ‚Äã

Si prefieres compilar Open vSwitch desde el c√≥digo fuente, puedes seguir las instrucciones oficiales proporcionadas por el proyecto. 
docs.openvswitch.org
‚Äã

¬øNecesitas asistencia adicional para configurar puentes (br0) o integrar Open vSwitch con Libvirt?

sudo ovs-vsctl show